---
title: 'Spark für Big Data Lernen'
date: '2020-08-26'
layout: '~/layouts/MarkdownLayout.astro'
---

Werden Sie Experte mit dem

# Spark Training für Big Data

## Auf einen Blick

- 3 Tage
- Monatlich und Individuell
- Technisches Training
- Remote und vor Ort verfügbar

Meistern Sie mit unserem Spark Training Ihre Datenanalyse mit Text Mining und Machine Learning Methoden mit Hands-On Übungen. Profitieren Sie von Projekterfahrung unserer Experten und Cloud-Architekten mit Erfahrung direkt aus der Industrie.

[Anfragen](#sec1)

Einer der nachgefragtesten Skillsets zu Zeiten von Big Data ist die Fähigkeit, große Datenmengen zu analysieren. In diesem interaktiven Kurs vermittelt Thinkport die notwendigen Grundlagen, um eigenständig mit Apache Spark Datenauswertungen durchführen zu können. Außerdem werden Performance-Tipps gezeigt und erklärt, die Ihnen helfen, große Datenmengen viel effizienter und günstiger zu verarbeiten. Das Spark Big Data Training ist plattformunabhängig und je nachdem, für welche Plattform Sie sich interessieren (AWS, Azure oder On-premise), umfasst die Agenda entsprechende Teile der Umgebung.

[](#linksection)[Linkedin](https://www.linkedin.com/company/11759873) [Instagram](https://www.instagram.com/thinkport/) [Youtube](https://www.youtube.com/channel/UCnke3WYRT6bxuMK2t4jw2qQ) [Envelope](mailto:tdrechsel@thinkport.digital)

## Termine

25.06. - 27.06.24 Spark Training für Big Data

03.07 - 05.07.24 Spark Training für Big Data

26.08. - 28.08.24 Spark Training für Big Data

\* individuelle Termine möglich

## Preis

2.200 € zzgl. MwSt.

## Lernerfolge

Teilnehmer können nach Abschluss...

- Große Datensätze und -mengen mit Apache Spark analysieren
- Mit Apache Spark Streaming Daten in Echtzeit sammeln und analysieren  
   (Hands-on Projekt: Twitter Daten)

- Anhand von Text Mining und Machine Learning Methoden Daten auswerten (Hands-on Projekt: Twitter Daten)
- Performance des bestehenden Clusters verbessern und mehr um Spark Best Practices lernen
- Eine Plattform für automatische Analysen in Microsoft Azure / AWS bauen

## Zielgruppe

Der Kurs ist geeignet für Personen mit...

- Grundkenntnissen in Python und dem Willen diese im Bezug auf Big Data zu vertiefen
- Erfahrungen in einer anderen Programmiersprache und Interesse Apache Spark kennenzulernen

## Aufbau

[Spark für Big Data im Handumdrehen lernen](https://www.hashicorp.com/)

### Grundlagen Tag I

- Überblick: Was ist Big Data
- Spark Grundlagen I: RDD
- Lab: Anwendung von Dataframes
- Überblick: Microsoft Azure
- Python Crashkurs (optional)

### Streaming Tag II

- Überblick: Was ist Streaming
- Spark Grundlagen II: Spark Streaming
- Day-Lab: Twitter Streaming mit Spark (Datenabzug, Auswertung mit Spark Streaming, Speichern in einem Datalake, und Erstellen einer Data Pipeline)

### Textmining Tag III

- Überblick: Machine Learning und Text Mining
- Machine Learning mit MLlib
- Day-Lab: Text Mining mit Machine Learning Modellen (Regression, Random Forest), Tokenization und Sentimentanalysis

- Überblick: Was ist Big Data
- Spark Grundlagen I: RDD
- Lab: Anwendung von Dataframes
- Überblick: Microsoft Azure
- Python Crashkurs (optional)

- Überblick: Was ist Streaming
- Spark Grundlagen II: Spark Streaming
- Day-Lab: Twitter Streaming mit Spark (Datenabzug, Auswertung mit Spark Streaming, Speichern in einem Datalake, und Erstellen einer Data Pipeline)

- Überblick: Machine Learning und Text Mining
- Machine Learning mit MLlib
- Day-Lab: Text Mining mit Machine Learning Modellen (Regression, Random Forest), Tokenization und Sentimentanalysis

![Sechs Personen, die an einem Tisch sitzen und offenbar verhandeln oder über Geschäfte sprechen.](images/DSC01530-1024x683.jpg)

## Erfolge

Die Lerninhalte unseres Spark Big Data Trainings werden von unseren Experten spannend und eingänglich vermittelt. Anhand eines real-world Use Case lernen Sie Spark als Data Analyse-Tool kennen.

## Kontakt

Erfahren Sie mehr zu diesem Training in einem persönlichen Gespräch

Sie setzen mit uns individuelle Schwerpunkte und erhalten ein zugeschnittenes Angebot für Ihre Anforderungen 24h nach dem Termin

Termin vereinbaren

## FAQs

Hier finden Sie eine kurze Zusammenstellung von oft gestellten Fragen und den dazugehörigen Antworten.

Was versteht man unter Big Data?

Unter Big Data versteht man die Verarbeitung von großen und komplexen Datenmengen, die herkömmliche Datenverarbeitungstechnologien übersteigen. Diese Datenmengen können aus verschiedenen Quellen stammen, wie beispielsweise soziale Netzwerke, Sensoren, Mobilgeräte oder Transaktionssysteme.

Die Charakteristiken von Big Data sind auch durch die sogenannten «3Vs» beschrieben:

- - **Volumen**: die Menge an Daten, die verarbeitet werden müssen, ist sehr groß
    - **Vielfalt**: die Daten können aus verschiedenen Quellen und in unterschiedlichen Formaten stammen
    - **Geschwindigkeit**: die Daten müssen schnell verarbeitet werden, um Echtzeit-Ergebnisse zu liefern.

Die Verarbeitung von Big Data erfordert spezielle Technologien und Tools wie NoSQL-Datenbanken, verteilte Dateisysteme, Hadoop, Spark und Machine Learning-Algorithmen.

Was ist Apache Spark? Apache Spark ist ein Framework für die verteilte Datenverarbeitung, das speziell für die Verarbeitung von Big Data entwickelt wurde. Es ist in der Programmiersprache Scala geschrieben und wird von der Apache Software Foundation entwickelt. Spark bietet eine schnelle und effiziente Möglichkeit, große Datenmengen in Echtzeit zu verarbeiten und zu analysieren. Spark hat sich zu einem der am häufigsten verwendeten Big-Data-Frameworks entwickelt und wird in verschiedenen Anwendungsbereichen eingesetzt, wie beispielsweise in der Finanzindustrie, der Gesundheitsversorgung, der Analyse von sozialen Netzwerken und im Internet der Dinge. Wie funktioniert Apache Spark?

Spark ist darauf ausgelegt, parallele Verarbeitung auf einem Cluster von Computern auszuführen, um eine schnelle und skalierbare Verarbeitung von Daten zu ermöglichen. Es bietet eine breite Palette von APIs für die Verarbeitung von Daten in verschiedenen Formaten, einschließlich strukturierten Daten, Texten, Graphen und maschinellem Lernen.

Ein weiteres Merkmal von Spark ist die Fähigkeit, Daten im Arbeitsspeicher zu halten, was es ermöglicht, die Daten in Echtzeit zu verarbeiten und schnelle Abfragen durchzuführen. Darüber hinaus unterstützt Spark die Integration mit anderen Big-Data-Ökosystemen wie Hadoop und NoSQL-Datenbanken.

Welche Vorteile bringt Apache Spark?

Apache Spark bietet eine Reihe von Vorteilen, die es zu einer beliebten Wahl für die Verarbeitung von Big Data machen. Hier sind einige der wichtigsten Vorteile von Apache Spark:

- - Schnelle Verarbeitung von großen Datenmengen
    - Flexibilität (breite Palette von APIs und Bibliotheken für die Verarbeitung)
    - Echtzeit-Verarbeitung
    - Skalierbarkeit
    - Integration mit anderen Big-Data-Technologien
    - In-Memory-Verarbeitung

Insgesamt bietet Apache Spark eine leistungsstarke und flexible Plattform für die Verarbeitung von Big Data und wird von vielen Unternehmen und Organisationen für die Verarbeitung von Daten in verschiedenen Anwendungsbereichen eingesetzt.

## Weitere Trainings
